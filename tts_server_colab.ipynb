{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0938286",
   "metadata": {},
   "source": [
    "# TTS Server on Google Colab\n",
    "\n",
    "This notebook sets up a Flask-based Text-to-Speech server using Parler TTS on Google Colab.\n",
    "\n",
    "## Features\n",
    "- Runs a local TTS server accessible via ngrok tunnel\n",
    "- Uses Parler TTS model from Hugging Face\n",
    "- Optimized for GPU acceleration\n",
    "- REST API for text-to-speech synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32637526",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q flask pyngrok\n",
    "!pip install -q \"parler-tts @ git+https://github.com/huggingface/parler-tts.git\"\n",
    "!pip install -q transformers accelerate\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d2867",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47796536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available. Using CPU (slower performance).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259e27b",
   "metadata": {},
   "source": [
    "## Step 3: Clone Model from Hugging Face (Optional)\n",
    "\n",
    "Clone the model repository to local storage. This is useful if you want to:\n",
    "- Save the model for later use\n",
    "- Avoid re-downloading on reconnect\n",
    "- Upload to Google Drive for persistent storage\n",
    "\n",
    "**Note:** Skip this step if you want to load directly from HF (faster first-time setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4732af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clone the model from Hugging Face\n",
    "# Uncomment the following lines if you want to clone the model locally\n",
    "\n",
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/ai4bharat/indic-parler-tts ./indic-parler-tts\n",
    "\n",
    "# If you cloned the model, set LOCAL_MODEL_PATH to the cloned directory\n",
    "# LOCAL_MODEL_PATH = \"./indic-parler-tts\"\n",
    "\n",
    "# Otherwise, load directly from Hugging Face (recommended for Colab)\n",
    "LOCAL_MODEL_PATH = None  # Set to \"./indic-parler-tts\" if you cloned locally\n",
    "\n",
    "print(\"‚úÖ Model path configured!\")\n",
    "if LOCAL_MODEL_PATH:\n",
    "    print(f\"   Using local model: {LOCAL_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"   Will load directly from Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ca6dc",
   "metadata": {},
   "source": [
    "## Step 4: Load the TTS Model\n",
    "\n",
    "This will download the Parler TTS model from Hugging Face (or load from local path if cloned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"ai4bharat/indic-parler-tts\"  # AI4Bharat's Indic Parler TTS (supports Indian languages)\n",
    "\n",
    "# Use local path if set, otherwise load from HF\n",
    "MODEL_PATH = LOCAL_MODEL_PATH if LOCAL_MODEL_PATH else MODEL_NAME\n",
    "\n",
    "# Performance optimizations\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "torch.set_num_interop_threads(os.cpu_count())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device)\n",
    "\n",
    "# Enable optimizations\n",
    "if torch.cuda.is_available():\n",
    "    model = model.half()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"‚úÖ GPU optimizations enabled: FP16 precision, cuDNN auto-tuning\")\n",
    "else:\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"‚úÖ CPU optimizations enabled\")\n",
    "\n",
    "model.eval()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "sample_rate = model.config.sampling_rate\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Sample rate: {sample_rate} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10799102",
   "metadata": {},
   "source": [
    "## Step 5: Test the Model (Optional)\n",
    "\n",
    "Let's test the model with a simple text-to-speech conversion. Try with both English and Indian languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720cadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Test synthesis\n",
    "test_text = \"Hello! This is a test of the text to speech system.\"\n",
    "description = \"A clear, natural voice with moderate pace and good pronunciation.\"\n",
    "\n",
    "print(f\"Generating audio for: '{test_text}'\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_input_ids = tokenizer(test_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    generation = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        prompt_input_ids=prompt_input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "\n",
    "print(\"‚úÖ Audio generated!\")\n",
    "display(Audio(audio_arr, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86393bc",
   "metadata": {},
   "source": [
    "## Step 6: Create Flask Server\n",
    "\n",
    "Now we'll create the Flask server with the TTS endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86576a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, send_file\n",
    "from io import BytesIO\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/synthesize', methods=['POST'])\n",
    "def synthesize():\n",
    "    data = request.get_json()\n",
    "    text = data.get('text')\n",
    "    \n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "\n",
    "    print(f\"Synthesizing: {text}\")\n",
    "\n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            # Define prompt and description\n",
    "            prompt = text\n",
    "            description = data.get('description', \"A clear, natural voice with moderate pace.\")\n",
    "\n",
    "            # Tokenize inputs\n",
    "            input_ids = tokenizer(\n",
    "                description, return_tensors=\"pt\"\n",
    "            ).input_ids.to(device)\n",
    "            prompt_input_ids = tokenizer(\n",
    "                prompt, return_tensors=\"pt\"\n",
    "            ).input_ids.to(device)\n",
    "\n",
    "            # Generate audio\n",
    "            generation = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                prompt_input_ids=prompt_input_ids,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            \n",
    "            audio_arr = generation.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Normalize to int16 PCM\n",
    "        if np.max(np.abs(audio_arr)) == 0:\n",
    "            scaled_audio = np.zeros_like(audio_arr, dtype=np.int16)\n",
    "        else:\n",
    "            scaled_audio = (\n",
    "                (audio_arr / np.max(np.abs(audio_arr))) * 32767\n",
    "            ).astype(np.int16)\n",
    "\n",
    "        pcm_data = scaled_audio.tobytes()\n",
    "        \n",
    "        return send_file(\n",
    "            BytesIO(pcm_data),\n",
    "            mimetype='application/octet-stream'\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during synthesis: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/info', methods=['GET'])\n",
    "def info():\n",
    "    return jsonify({\"sample_rate\": sample_rate, \"channels\": 1, \"model\": MODEL_NAME})\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"healthy\", \"device\": device})\n",
    "\n",
    "print(\"‚úÖ Flask server configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f0a9d",
   "metadata": {},
   "source": [
    "## Step 7: Setup ngrok for Public Access\n",
    "\n",
    "ngrok creates a public URL that tunnels to your Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# Optional: Set your ngrok auth token for better limits\n",
    "# Get free token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "# ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")\n",
    "\n",
    "# Start Flask in a background thread\n",
    "def run_flask():\n",
    "    app.run(port=5001, threaded=True)\n",
    "\n",
    "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(5001)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TTS SERVER IS RUNNING!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüì° Public URL: {public_url}\")\n",
    "print(f\"\\nEndpoints:\")\n",
    "print(f\"  ‚Ä¢ Health check: {public_url}/health\")\n",
    "print(f\"  ‚Ä¢ Server info:  {public_url}/info\")\n",
    "print(f\"  ‚Ä¢ Synthesize:   {public_url}/synthesize (POST)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Example cURL command:\")\n",
    "print(f'''curl -X POST {public_url}/synthesize \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\n",
    "    \"text\": \"Hello, this is a test.\",\n",
    "    \"description\": \"A clear, natural voice.\"\n",
    "  }}' \\\\\n",
    "  --output audio.raw''')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è Keep this cell running to maintain the server!\")\n",
    "print(\"   Press the stop button to shut down the server.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48d988",
   "metadata": {},
   "source": [
    "## Step 8: Test the API (Optional)\n",
    "\n",
    "Test your deployed server directly from the notebook. Try both English and Indian languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34357f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "# Test the server with English\n",
    "print(\"Testing with English text...\")\n",
    "test_url = f\"{public_url}/synthesize\"\n",
    "test_payload = {\n",
    "    \"text\": \"Welcome to the TTS server running on Google Colab!\",\n",
    "    \"description\": \"An enthusiastic, clear voice with moderate pace.\"\n",
    "}\n",
    "\n",
    "print(f\"Testing server at: {test_url}\")\n",
    "response = requests.post(test_url, json=test_payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convert raw PCM bytes to numpy array\n",
    "    audio_data = np.frombuffer(response.content, dtype=np.int16)\n",
    "    # Convert int16 to float for playback\n",
    "    audio_float = audio_data.astype(np.float32) / 32767.0\n",
    "    \n",
    "    print(\"‚úÖ Audio received successfully!\")\n",
    "    display(Audio(audio_float, rate=sample_rate))\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "# Test with Hindi (optional)\n",
    "print(\"\\nTesting with Hindi text...\")\n",
    "hindi_payload = {\n",
    "    \"text\": \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•à‡•§\",\n",
    "    \"description\": \"A clear Hindi voice with moderate pace.\"\n",
    "}\n",
    "\n",
    "response_hindi = requests.post(test_url, json=hindi_payload)\n",
    "if response_hindi.status_code == 200:\n",
    "    audio_data_hindi = np.frombuffer(response_hindi.content, dtype=np.int16)\n",
    "    audio_float_hindi = audio_data_hindi.astype(np.float32) / 32767.0\n",
    "    print(\"‚úÖ Hindi audio received successfully!\")\n",
    "    display(Audio(audio_float_hindi, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1601406",
   "metadata": {},
   "source": [
    "## Stop the Server\n",
    "\n",
    "Run this cell when you want to stop the server and close the ngrok tunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close ngrok tunnels\n",
    "ngrok.kill()\n",
    "print(\"‚úÖ Server stopped and ngrok tunnel closed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
